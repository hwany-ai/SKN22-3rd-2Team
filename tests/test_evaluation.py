"""
ì‡¼íŠ¹í—ˆ (Short-Cut) v3.0 - DeepEval RAG Quality Tests
=====================================================
RAG íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸ (Faithfulness, Answer Relevancy)

Metrics:
1. FaithfulnessMetric - ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ì§€ ê²€ì¦
2. AnswerRelevancyMetric - ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ ê²€ì¦

Team: ë€¨ğŸ’•
"""

import pytest
import asyncio
import sys
import os
from pathlib import Path
from typing import List, Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Check for required environment variables
if not os.environ.get("OPENAI_API_KEY"):
    pytest.skip(
        "OPENAI_API_KEY not set. Skipping DeepEval tests.",
        allow_module_level=True
    )

# DeepEval imports
try:
    from deepeval import assert_test
    from deepeval.test_case import LLMTestCase
    from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric
except ImportError:
    pytest.skip(
        "deepeval not installed. Run: pip install deepeval",
        allow_module_level=True
    )

# Import PatentAgent
from patent_agent import PatentAgent


# =============================================================================
# Golden Dataset - AI/NLP Domain Test Cases
# =============================================================================
# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ëŠ” ìš°ë¦¬ ë°ì´í„°ì˜ ë„ë©”ì¸ í‚¤ì›Œë“œì— ë§ê²Œ ì„¤ê³„ë¨:
# - retrieval augmented generation
# - large language model
# - neural information retrieval
# - semantic search
# - document embedding
# - transformer attention
# - knowledge graph reasoning
# - prompt engineering

GOLDEN_DATASET: List[Dict[str, Any]] = [
    {
        "id": "test_001",
        "name": "RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
        "query": """
        Retrieval Augmented Generation ê¸°ìˆ ì„ í™œìš©í•œ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
        ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ 
        ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì— ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤.
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(Dense + Sparse)ê³¼ RRF ìœµí•©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """,
        "expected_topics": ["retrieval", "embedding", "vector", "search"],
    },
    {
        "id": "test_002",
        "name": "Semantic Search ì—”ì§„",
        "query": """
        Neural information retrieval ê¸°ë°˜ì˜ semantic search ì—”ì§„ì…ë‹ˆë‹¤.
        Transformer ëª¨ë¸ë¡œ ë¬¸ì„œì™€ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•˜ê³ ,
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ì „í†µì ì¸ í‚¤ì›Œë“œ ê²€ìƒ‰ë³´ë‹¤ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """,
        "expected_topics": ["semantic", "transformer", "embedding", "neural"],
    },
    {
        "id": "test_003",
        "name": "LLM Fine-tuning ì‹œìŠ¤í…œ",
        "query": """
        Large Language Modelì„ íŠ¹ì • ë„ë©”ì¸ì— fine-tuningí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
        íš¨ìœ¨ì ì¸ inferenceë¥¼ ìœ„í•´ quantization ê¸°ë²•ì„ ì ìš©í•˜ê³ ,
        prompt engineeringìœ¼ë¡œ ìµœì í™”ëœ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """,
        "expected_topics": ["language model", "fine-tuning", "inference", "prompt"],
    },
]


# =============================================================================
# Test Configuration
# =============================================================================

# Metric thresholds
FAITHFULNESS_THRESHOLD = 0.7
RELEVANCY_THRESHOLD = 0.7

# Evaluation model
EVAL_MODEL = "gpt-4o-mini"  # Cost-effective for testing


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture(scope="module")
def event_loop():
    """Create event loop for async tests."""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="module")
def patent_agent():
    """Initialize PatentAgent (expensive, so module-scoped)."""
    try:
        agent = PatentAgent()
        return agent
    except Exception as e:
        pytest.skip(f"Failed to initialize PatentAgent: {e}")


# =============================================================================
# Helper Functions
# =============================================================================

def extract_retrieval_context(search_results: List[Dict]) -> List[str]:
    """
    Extract retrieval context from search results.
    
    Combines abstract and claims from each patent for full context.
    This is critical for DeepEval FaithfulnessMetric accuracy.
    """
    context = []
    for result in search_results:
        patent_id = result.get("patent_id", "Unknown")
        title = result.get("title", "")
        abstract = result.get("abstract", "")
        claims = result.get("claims", "")
        
        # Build comprehensive context string
        context_parts = [f"Patent {patent_id}: {title}"]
        
        if abstract:
            # Truncate abstract if too long (DeepEval has token limits)
            abstract_truncated = abstract[:1500] if len(abstract) > 1500 else abstract
            context_parts.append(f"Abstract: {abstract_truncated}")
        
        if claims:
            # Truncate claims if too long
            claims_truncated = claims[:2000] if len(claims) > 2000 else claims
            context_parts.append(f"Claims: {claims_truncated}")
        
        if result.get("grading_reason"):
            context_parts.append(f"Relevance: {result['grading_reason']}")
        
        context.append(" | ".join(context_parts))
    
    return context


def extract_actual_output(analysis: Dict) -> str:
    """
    Extract actual output string from analysis result.
    
    Combines key analysis sections into a single string.
    """
    parts = []
    
    # Similarity
    sim = analysis.get("similarity", {})
    if sim.get("summary"):
        parts.append(f"ìœ ì‚¬ë„ ë¶„ì„: {sim['summary']} (ì ìˆ˜: {sim.get('score', 'N/A')})")
    
    # Infringement
    inf = analysis.get("infringement", {})
    if inf.get("summary"):
        parts.append(f"ì¹¨í•´ ë¦¬ìŠ¤í¬: {inf['summary']} (ë ˆë²¨: {inf.get('risk_level', 'N/A')})")
    
    # Avoidance
    avoid = analysis.get("avoidance", {})
    if avoid.get("summary"):
        parts.append(f"íšŒí”¼ ì „ëµ: {avoid['summary']}")
    
    # Conclusion
    if analysis.get("conclusion"):
        parts.append(f"ê²°ë¡ : {analysis['conclusion']}")
    
    return " | ".join(parts) if parts else "No analysis available"


async def run_agent_analysis(agent: PatentAgent, query: str) -> Dict[str, Any]:
    """Run PatentAgent analysis asynchronously."""
    return await agent.analyze(query, use_hybrid=True, stream=False)


# =============================================================================
# DeepEval Metric Instances
# =============================================================================

@pytest.fixture(scope="module")
def faithfulness_metric():
    """Create FaithfulnessMetric instance."""
    return FaithfulnessMetric(
        threshold=FAITHFULNESS_THRESHOLD,
        model=EVAL_MODEL,
        include_reason=True,
    )


@pytest.fixture(scope="module")
def relevancy_metric():
    """Create AnswerRelevancyMetric instance."""
    return AnswerRelevancyMetric(
        threshold=RELEVANCY_THRESHOLD,
        model=EVAL_MODEL,
        include_reason=True,
    )


# =============================================================================
# Test Class: RAG Quality Evaluation
# =============================================================================

class TestRAGQuality:
    """
    RAG íŒŒì´í”„ë¼ì¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤.
    
    DeepEvalì˜ FaithfulnessMetricê³¼ AnswerRelevancyMetricì„ ì‚¬ìš©í•˜ì—¬
    PatentAgentì˜ ë¶„ì„ ê²°ê³¼ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    
    @pytest.mark.asyncio
    @pytest.mark.parametrize("test_case", GOLDEN_DATASET, ids=lambda tc: tc["id"])
    async def test_rag_quality(
        self,
        test_case: Dict[str, Any],
        patent_agent: PatentAgent,
        faithfulness_metric: FaithfulnessMetric,
        relevancy_metric: AnswerRelevancyMetric,
    ):
        """
        RAG í’ˆì§ˆ í…ŒìŠ¤íŠ¸: Faithfulness + Answer Relevancy.
        
        Args:
            test_case: Golden datasetì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            patent_agent: PatentAgent ì¸ìŠ¤í„´ìŠ¤
            faithfulness_metric: ì¶©ì‹¤ë„ ë©”íŠ¸ë¦­
            relevancy_metric: ê´€ë ¨ì„± ë©”íŠ¸ë¦­
        """
        print(f"\n{'='*60}")
        print(f"ğŸ§ª Test Case: {test_case['name']}")
        print(f"{'='*60}")
        
        query = test_case["query"].strip()
        
        # Step 1: Run PatentAgent analysis
        try:
            result = await run_agent_analysis(patent_agent, query)
        except Exception as e:
            pytest.fail(f"PatentAgent.analyze() failed: {e}")
        
        # Check for errors
        if "error" in result:
            pytest.skip(f"No patents found: {result['error']}")
        
        # Step 2: Extract components for LLMTestCase
        search_results = result.get("search_results", [])
        analysis = result.get("analysis", {})
        
        # Input: User's idea
        input_text = query
        
        # Actual Output: Analysis conclusion/summary
        actual_output = extract_actual_output(analysis)
        
        # Retrieval Context: Patent abstracts/claims
        retrieval_context = extract_retrieval_context(search_results)
        
        print(f"\nğŸ“¥ Input Query: {input_text[:100]}...")
        print(f"ğŸ“¤ Actual Output: {actual_output[:200]}...")
        print(f"ğŸ“š Context Count: {len(retrieval_context)}")
        
        # Step 3: Create LLMTestCase
        llm_test_case = LLMTestCase(
            input=input_text,
            actual_output=actual_output,
            retrieval_context=retrieval_context,
        )
        
        # Step 4: Measure and assert with DeepEval metrics
        print("\nğŸ” Running DeepEval metrics...")
        
        # Measure Faithfulness (this computes the score)
        faithfulness_metric.measure(llm_test_case)
        faith_score = faithfulness_metric.score
        faith_reason = faithfulness_metric.reason
        print(f"   ğŸ“Š Faithfulness Score: {faith_score:.2f} (threshold: {FAITHFULNESS_THRESHOLD})")
        if faith_reason:
            print(f"      Reason: {faith_reason[:150]}...")
        
        # Assert Faithfulness
        if faith_score < FAITHFULNESS_THRESHOLD:
            raise AssertionError(f"Faithfulness score {faith_score:.2f} < {FAITHFULNESS_THRESHOLD}")
        print(f"   âœ… Faithfulness: PASSED")
        
        # Measure Answer Relevancy
        relevancy_metric.measure(llm_test_case)
        rel_score = relevancy_metric.score
        rel_reason = relevancy_metric.reason
        print(f"   ğŸ“Š Answer Relevancy Score: {rel_score:.2f} (threshold: {RELEVANCY_THRESHOLD})")
        if rel_reason:
            print(f"      Reason: {rel_reason[:150]}...")
        
        # Assert Relevancy
        if rel_score < RELEVANCY_THRESHOLD:
            raise AssertionError(f"Answer Relevancy score {rel_score:.2f} < {RELEVANCY_THRESHOLD}")
        print(f"   âœ… Answer Relevancy: PASSED")
        
        print(f"\n{'='*60}")
        print(f"âœ… Test Case '{test_case['name']}' PASSED")
        print(f"{'='*60}")
    



# =============================================================================
# Standalone Test Functions (Alternative to Class)
# =============================================================================

@pytest.mark.asyncio
async def test_single_query_quality(patent_agent: PatentAgent):
    """
    ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ë¹ ë¥¸ í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸.
    
    CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ê²½ëŸ‰ í…ŒìŠ¤íŠ¸.
    """
    query = "ìì—°ì–´ ì²˜ë¦¬ ê¸°ë°˜ íŠ¹í—ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ"
    
    result = await patent_agent.analyze(query, use_hybrid=True)
    
    # Basic assertions (not DeepEval)
    assert "analysis" in result, "Analysis should be present in result"
    assert "search_results" in result, "Search results should be present"
    assert result.get("analysis", {}).get("conclusion"), "Conclusion should not be empty"
    
    print(f"âœ… Single query test passed")
    print(f"   - Found {len(result.get('search_results', []))} patents")
    print(f"   - Similarity score: {result['analysis']['similarity'].get('score')}")


# =============================================================================
# Run Tests
# =============================================================================

if __name__ == "__main__":
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "-x",  # Stop on first failure
        "--asyncio-mode=auto",
    ])
